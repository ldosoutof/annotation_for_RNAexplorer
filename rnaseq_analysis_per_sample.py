#!/usr/bin/env python3
"""
RNA-Seq Analysis Pipeline - Per Sample Output
Processes FRASER2 and OUTRIDER outputs and generates one file per sample per tool

Stratégie de parallélisation :
  Étape 1 — Chargement des fichiers  : ThreadPoolExecutor  (I/O-bound, libère le GIL)
  Étape 2 — Annotation + écriture    : ProcessPoolExecutor (CPU-bound par sample,
                                        contourne le GIL pour pandas)
  FRASER et OUTRIDER sont traités dans des pools distincts mais peuvent tourner
  simultanément si les ressources le permettent.
"""

import argparse
import json
import os
import zipfile
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed
from datetime import datetime
from pathlib import Path
import logging
import sys

import pandas as pd

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

DEFAULT_WORKERS = min(os.cpu_count() or 4, 16)


# =============================================================================
# Fonctions module-level (picklables -> ProcessPoolExecutor)
# =============================================================================

def _process_and_save_sample(args):
    """
    Traite et sauvegarde les donnees d'UN sample.
    Recoit les donnees du sample deja filtrees (en dict pour la picklabilite),
    applique les annotations et ecrit le fichier TSV.
    Execute dans un process worker -> vraie parallelisation CPU.
    """
    (
        sample_full,
        sample_dict,
        tool_name,
        files_dir,
        gtf_dict,
        gnomad_dict,
        mendeliome_dict,
        gene_col,
    ) = args

    sample_short = sample_full.split('.')[0]
    filepath = Path(files_dir) / f"{sample_short}.{tool_name}.tab"

    df = pd.DataFrame(sample_dict)

    # -- Annotation GTF -------------------------------------------------------

    if tool_name == 'outrider' and gtf_dict:
        by_gene = gtf_dict.get('by_gene', gtf_dict)
    
        def _gtf_field(gid, field):
            if pd.isna(gid):
                return None
            return by_gene.get(str(gid).split('.')[0], {}).get(field)
    
        for field in ('gene_name', 'chrom', 'start', 'end', 'strand'):
            df[field] = df['geneID'].apply(lambda x, f=field: _gtf_field(x, f))
    
        # Remplacer geneID par le gene_id du GTF (avec numéro de version)
        df['geneID'] = df['geneID'].apply(
            lambda gid: _gtf_field(gid, 'gene_id') or gid
        )
        # Renommer la colonne
        df.rename(columns={'geneID': 'gene_id'}, inplace=True)
    
        if 'gene_name' in df.columns and df['gene_name'].notna().any():
            gene_col = 'gene_name'

    elif tool_name == 'fraser':
        df['chrom'] = df.get('seqnames', pd.Series(dtype=str))
    
        # ← NOUVEAU : résolution gene_name par overlap de coordonnées
        if gtf_dict and 'by_chrom' in gtf_dict:
            by_chrom = gtf_dict['by_chrom']
    
            def _resolve_fraser_gene(row):
                chrom   = str(row.get('seqnames', ''))
                start   = row.get('start', 0)
                end     = row.get('end', 0)
                for e in by_chrom.get(chrom, []):
                    if e['start'] <= end and e['end'] >= start:
                        return e['gene_name'], e['gene_id']
                return None, None
    
            df[['gene_name', 'gene_id']] = df.apply(
                _resolve_fraser_gene, axis=1, result_type='expand'
            )
    
        if 'gene_name' in df.columns and df['gene_name'].notna().any():
            gene_col = 'gene_name'
            
    # -- Annotation gnomAD ----------------------------------------------------
    if gnomad_dict and gene_col in df.columns:
        for metric in ('pLI', 'oe_lof', 'lof_z', 'mis_z', 'syn_z',
                       'constraint_flag', 'oe_mis', 'oe_syn'):
            df[metric] = df[gene_col].map(
                lambda g, m=metric: gnomad_dict.get(str(g), {}).get(m)
            )

    # -- Annotation Mendeliome ------------------------------------------------
    if mendeliome_dict and gene_col in df.columns:
        for col in ('Mode_Of_Inheritance', 'Phenotypes'):
            df[col] = df[gene_col].map(
                lambda g, c=col: mendeliome_dict.get(str(g), {}).get(c)
            )

    # -- Selection et ordre des colonnes de sortie ----------------------------
    if tool_name == 'fraser':
        ordered = [
            'seqnames', 'start', 'end', 'width', 'strand', 'sampleID',
            'type', 'pValue', 'padjust', 'psiValue', 'deltaPsi', 'counts',
            'totalCounts', 'meanCounts', 'meanTotalCounts', 'nonsplitCounts',
            'nonsplitProportion', 'nonsplitProportion_99quantile',
            'gene_name', 'gene_id', 'chrom',
            'pLI', 'oe_lof', 'lof_z', 'mis_z',
            'Mode_Of_Inheritance', 'Phenotypes',
        ]
    else:
        ordered = [
            'gene_id', 'sampleID', 'pValue', 'padjust', 'zScore', 'l2fc',
            'rawcounts', 'meanRawcounts', 'normcounts', 'meanCorrected',
            'theta', 'aberrant', 'AberrantBySample', 'AberrantByGene',
            'padj_rank', 'gene_name', 'chrom', 'start', 'end', 'strand',
            'pLI', 'oe_lof', 'lof_z', 'mis_z',
            'Mode_Of_Inheritance', 'Phenotypes',
        ]

    out_cols = [c for c in ordered if c in df.columns]
    df[out_cols].to_csv(filepath, sep='\t', index=False)
    return filepath, len(df), sample_short


# =============================================================================
# Helpers de conversion dict (picklables)
# =============================================================================

def _gtf_to_dict(gtf_df):
    if gtf_df is None:
        return {}
    gtf_df = gtf_df.copy()
    gtf_df['gene_id_clean'] = gtf_df['gene_id'].str.split('.').str[0]
    
    by_chrom = {}
    for _, row in gtf_df.iterrows():
        chrom = row['chrom'].replace('chr', '')
        by_chrom.setdefault(chrom, []).append({
            'gene_id':   row['gene_id_clean'],
            'gene_name': row['gene_name'],
            'start':     row['start'],
            'end':       row['end'],
            'strand':    row['strand'],
        })
    
    # Dict par gene_id (OUTRIDER, inchangé)
    by_gene = (
        gtf_df.set_index('gene_id_clean')
        [['gene_id','gene_name', 'chrom', 'start', 'end', 'strand']]
        .to_dict('index')
    )
    
    return {'by_gene': by_gene, 'by_chrom': by_chrom}

def _gnomad_to_dict(gnomad_df):
    if gnomad_df is None:
        return {}
    wanted = ["pLI", "oe_lof", "lof_z", "mis_z", "syn_z",
              "constraint_flag", "oe_mis", "oe_syn"]
    cols = [c for c in wanted if c in gnomad_df.columns]
    df = gnomad_df[["gene"] + cols].copy()
    if "pLI" in df.columns:
        df = df.sort_values("pLI", ascending=False)
    df = df.drop_duplicates(subset="gene", keep="first")
    return df.set_index("gene")[cols].to_dict("index")


def _mendeliome_to_dict(mendel_df):
    if mendel_df is None:
        return {}
    cols = [c for c in ('confidence_level', 'Mode_Of_Inheritance', 'Phenotypes')
            if c in mendel_df.columns]
    return mendel_df.set_index('gene_symbol')[cols].to_dict('index')


# =============================================================================
# Processeur principal
# =============================================================================

class RNASeqProcessorPerSample:
    """Traite et annote les resultats FRASER2/OUTRIDER avec une sortie par sample."""

    def __init__(self, fraser_file, outrider_file, samples_file, gtf_file,
                 output_dir, gnomad_file=None, mendeliome_file=None,
                 mode='samples', pvalue_filter=None, create_zip=True,
                 partial_match=False, workers=DEFAULT_WORKERS):

        self.fraser_file     = Path(fraser_file)     if fraser_file     else None
        self.outrider_file   = Path(outrider_file)   if outrider_file   else None
        self.samples_file    = Path(samples_file)    if samples_file    else None
        self.gtf_file        = Path(gtf_file)
        self.gnomad_file     = Path(gnomad_file)     if gnomad_file     else None
        self.mendeliome_file = Path(mendeliome_file) if mendeliome_file else None

        self.output_dir    = Path(output_dir)
        self.mode          = mode
        self.pvalue_filter = pvalue_filter
        self.create_zip    = create_zip
        self.partial_match = partial_match
        self.workers       = workers

        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.files_dir = self.output_dir / "per_sample_files"
        self.files_dir.mkdir(parents=True, exist_ok=True)

        self.samples         = None
        self.fraser_data     = None
        self.outrider_data   = None
        self.gtf_data        = None
        self.gnomad_data     = None
        self.mendeliome_data = None

        # Dicts picklables (calcules une seule fois avant le pool de process)
        self._gtf_dict        = None
        self._gnomad_dict     = None
        self._mendeliome_dict = None

    # -------------------------------------------------------------------------
    # Chargement
    # -------------------------------------------------------------------------

    def load_samples(self):
        if self.mode == 'all':
            logger.info("Mode : TOUS les samples")
            self.samples = None
            return None
        logger.info(f"Chargement des samples depuis {self.samples_file}")
        with open(self.samples_file) as f:
            self.samples = [l.strip() for l in f if l.strip()]
        logger.info(f"{len(self.samples)} samples charges")
        return self.samples

    def load_fraser(self):
        if self.fraser_file is None:
            return None
        logger.info(f"Chargement FRASER : {self.fraser_file}")
        self.fraser_data = pd.read_csv(self.fraser_file, sep='\t', low_memory=False)
        logger.info(f"  -> {len(self.fraser_data):,} enregistrements")
        return self.fraser_data

#    def load_outrider(self):
#        if self.outrider_file is None:
#            return None
#        logger.info(f"Chargement OUTRIDER : {self.outrider_file}")
#        self.outrider_data = pd.read_csv(self.outrider_file, sep='\t', low_memory=False)
#        logger.info(f"  -> {len(self.outrider_data):,} enregistrements")
#        return self.outrider_data

    def load_outrider(self):
        """Load OUTRIDER data"""
        logger.info(f"Loading OUTRIDER data from {self.outrider_file}")
        self.outrider_data = pd.read_csv(
            self.outrider_file,
            sep='\t',
            low_memory=False,
            index_col=0  # ignore la colonne d'index R
        )
        logger.info(f"Loaded {len(self.outrider_data)} OUTRIDER records")
        return self.outrider_data

    def load_gtf(self):
        logger.info(f"Chargement GTF : {self.gtf_file}")
        records = []
        with open(self.gtf_file) as f:
            for line in f:
                if line.startswith('#'):
                    continue
                fields = line.strip().split('\t')
                if len(fields) < 9 or fields[2] != 'gene':
                    continue
                attrs = {}
                for attr in fields[8].split(';'):
                    attr = attr.strip()
                    if not attr:
                        continue
                    kv = attr.split(' ', 1)
                    if len(kv) == 2:
                        attrs[kv[0]] = kv[1].strip('"')
                records.append({
                    'chrom':     fields[0],
                    'start':     int(fields[3]),
                    'end':       int(fields[4]),
                    'strand':    fields[6],
                    'gene_id':   attrs.get('gene_id'),
                    'gene_name': attrs.get('gene_name'),
                })
        self.gtf_data = pd.DataFrame(records)
        logger.info(f"  -> {len(self.gtf_data):,} genes")
        return self.gtf_data

    def load_gnomad(self):
        if self.gnomad_file is None:
            return None
        logger.info(f"Chargement gnomAD : {self.gnomad_file}")
        self.gnomad_data = pd.read_csv(self.gnomad_file, sep='\t', low_memory=False)
        logger.info(f"  -> {len(self.gnomad_data):,} genes")
        return self.gnomad_data

    def load_mendeliome(self):
        if self.mendeliome_file is None:
            return None
        if not self.mendeliome_file.exists():
            logger.warning(f"Mendeliome introuvable : {self.mendeliome_file}")
            return None
        logger.info(f"Chargement Mendeliome JSON : {self.mendeliome_file}")
        with open(self.mendeliome_file) as f:
            payload = json.load(f)
        version = payload.get("version", "?")
        records = []
        for entry in payload.get("genes", []):
            gd     = entry.get("gene_data", {})
            symbol = gd.get("gene_symbol") or gd.get("hgnc_symbol")
            if not symbol:
                continue
            phenotypes = entry.get("phenotypes", [])
            records.append({
                "gene_symbol":         symbol,
                "confidence_level":    str(entry.get("confidence_level", "")),
                "Mode_Of_Inheritance": entry.get("mode_of_inheritance", ""),
                "Phenotypes":          " | ".join(p for p in phenotypes if p),
            })
        self.mendeliome_data = (
            pd.DataFrame(records)
            .drop_duplicates(subset="gene_symbol")
        )
        n_green = (self.mendeliome_data["confidence_level"] == "3").sum()
        logger.info(
            f"  -> {len(self.mendeliome_data):,} genes uniques "
            f"(v{version}, {n_green} verts)"
        )
        return self.mendeliome_data

    def load_all_data(self):
        """Chargement parallele via ThreadPoolExecutor (I/O-bound)."""
        logger.info("Chargement parallele des fichiers...")
        tasks = {
            "GTF":        self.load_gtf,
            "gnomAD":     self.load_gnomad,
            "Mendeliome": self.load_mendeliome,
            "FRASER":     self.load_fraser,
            "OUTRIDER":   self.load_outrider,
        }
        errors = {}
        with ThreadPoolExecutor(max_workers=len(tasks)) as ex:
            futures = {ex.submit(fn): name for name, fn in tasks.items()}
            for future in as_completed(futures):
                name = futures[future]
                try:
                    future.result()
                    logger.info(f"  OK {name}")
                except Exception as e:
                    errors[name] = e
                    logger.error(f"  ERREUR {name} : {e}")

        if "GTF" in errors:
            raise RuntimeError(f"GTF indisponible : {errors['GTF']}")
        if errors:
            logger.warning(f"Donnees non chargees : {list(errors.keys())}")

        # Conversion en dicts picklables une seule fois pour tous les workers
        logger.info("Construction des dicts picklables pour ProcessPool...")
        self._gtf_dict        = _gtf_to_dict(self.gtf_data)
        self._gnomad_dict     = _gnomad_to_dict(self.gnomad_data)
        self._mendeliome_dict = _mendeliome_to_dict(self.mendeliome_data)
        logger.info(
            f"  GTF : {len(self._gtf_dict):,} | "
            f"gnomAD : {len(self._gnomad_dict):,} | "
            f"Mendeliome : {len(self._mendeliome_dict):,}"
        )

    # -------------------------------------------------------------------------
    # Filtrage samples
    # -------------------------------------------------------------------------

    def _get_matched_samples(self, data_samples):
        if self.mode == 'all':
            return list(data_samples)
        matched = []
        if self.partial_match:
            for ds in data_samples:
                if any(ls in ds for ls in self.samples):
                    matched.append(ds)
        else:
            s_set = set(self.samples)
            matched = [s for s in data_samples if s in s_set]
        logger.info(f"{len(matched)}/{len(self.samples or [])} samples trouves")
        found = set()
        for ds in matched:
            for ls in (self.samples or []):
                if (self.partial_match and ls in ds) or ls == ds:
                    found.add(ls)
        not_found = set(self.samples or []) - found
        if not_found:
            logger.warning(f"Samples non trouves : {sorted(not_found)}")
        return matched

    def _filter_data(self, data, label):
        if self.mode != 'all':
            matched = self._get_matched_samples(data['sampleID'].unique())
            data = data[data['sampleID'].isin(matched)].copy()
        if self.pvalue_filter is not None:
            n_before = len(data)
            data = data[data['padjust'] < self.pvalue_filter].copy()
            logger.info(f"Filtre p-value ({label}) : {n_before:,} -> {len(data):,}")
        logger.info(f"{label} filtre : {len(data):,} enregistrements")
        return data

    # -------------------------------------------------------------------------
    # Traitement parallele par sample (coeur du pipeline)
    # -------------------------------------------------------------------------

    def _run_tool_parallel(self, data, tool_name, gene_col):
        """
        Repartit les samples sur self.workers process.
        Chaque process recoit les donnees d'UN sample + les dicts de reference
        et effectue annotation + ecriture de facon totalement independante.
        -> Vraie parallelisation CPU : pas de GIL, pas de partage memoire.
        """
        unique_samples = data['sampleID'].unique()
        logger.info(
            f"Traitement {tool_name} : {len(unique_samples)} samples "
            f"sur {self.workers} workers"
        )

        tasks = [
            (
                sample,
                data[data['sampleID'] == sample].to_dict('list'),
                tool_name,
                str(self.files_dir),
                self._gtf_dict,
                self._gnomad_dict,
                self._mendeliome_dict,
                gene_col,
            )
            for sample in unique_samples
        ]

        saved_files = []
        with ProcessPoolExecutor(max_workers=self.workers) as executor:
            futures = {executor.submit(_process_and_save_sample, t): t[0]
                       for t in tasks}
            for future in as_completed(futures):
                sample = futures[future]
                try:
                    filepath, n_records, short = future.result()
                    saved_files.append(filepath)
                    logger.info(f"  OK {filepath.name} ({n_records} lignes)")
                except Exception as exc:
                    logger.error(f"  ERREUR {sample} : {exc}")

        logger.info(f"{len(saved_files)}/{len(unique_samples)} fichiers {tool_name} crees")
        return saved_files

    def process_fraser(self):
        if self.fraser_data is None:
            return []
        logger.info("--- FRASER ---")
        data = self._filter_data(self.fraser_data, 'FRASER')
        gene_col = 'hgncSymbol' if 'hgncSymbol' in data.columns else 'gene_name'
        return self._run_tool_parallel(data, 'fraser', gene_col)

    def process_outrider(self):
        if self.outrider_data is None:
            return []
        logger.info("--- OUTRIDER ---")
        data = self._filter_data(self.outrider_data, 'OUTRIDER')
        # gene_col sera recalcule dans le worker apres annotation GTF
        gene_col = 'geneID'
        return self._run_tool_parallel(data, 'outrider', gene_col)

    # -------------------------------------------------------------------------
    # ZIP
    # -------------------------------------------------------------------------

    def create_zip_archive(self, files_list):
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        zip_path  = self.output_dir / f"run_{timestamp}.zip"
        logger.info(f"Creation ZIP : {zip_path.name} ({len(files_list)} fichiers)")
        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zf:
            for fp in files_list:
                zf.write(fp, Path(fp).name)
        logger.info(f"ZIP cree : {zip_path}")
        return zip_path

    # -------------------------------------------------------------------------
    # Point d'entree
    # -------------------------------------------------------------------------

    def run(self):
        logger.info("=" * 60)
        logger.info("Pipeline RNA-Seq — sortie par sample")
        logger.info(f"Workers : {self.workers} | Mode : {self.mode.upper()}")
        logger.info("=" * 60)

        # Etape 1 : chargement parallele (threads, I/O-bound)
        self.load_samples()
        self.load_all_data()

        # Etape 2 : annotation + ecriture par sample (process, CPU-bound)
        fraser_files   = self.process_fraser()
        outrider_files = self.process_outrider()

        all_files = fraser_files + outrider_files

        if self.create_zip and all_files:
            zip_path = self.create_zip_archive(all_files)
            logger.info("=" * 60)
            logger.info("Pipeline termine !")
            logger.info(f"ZIP : {zip_path}")
            logger.info("=" * 60)
            return zip_path

        logger.info("=" * 60)
        logger.info("Pipeline termine !")
        logger.info(f"Fichiers : {self.files_dir}")
        logger.info("=" * 60)
        return all_files


# =============================================================================
# CLI autonome
# =============================================================================

def main():
    parser = argparse.ArgumentParser(
        description='Pipeline RNA-Seq — sortie par sample (FRASER2 + OUTRIDER)',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=f"""
Exemples :

  # Samples specifiques + ZIP
  %(prog)s --fraser fraser.tab --outrider outrider.tab --samples samples.txt \\
           --gtf genes.gtf --output results/

  # Tous les samples, filtre p-value, 8 workers
  %(prog)s --fraser fraser.tab --outrider outrider.tab --gtf genes.gtf \\
           --output results/ --mode all --pvalue 0.05 --workers 8

  # Avec gnomAD et Mendeliome Australia (JSON)
  %(prog)s --fraser fraser.tab --outrider outrider.tab --samples samples.txt \\
           --gtf genes.gtf --gnomad gnomad.txt \\
           --mendeliome references/mendeliome_australia.json \\
           --partial-match --output results/

Workers par defaut : {DEFAULT_WORKERS} (base sur les CPU disponibles)
        """
    )

    parser.add_argument('--fraser',   help='Fichier FRASER2 (TSV)')
    parser.add_argument('--outrider', help='Fichier OUTRIDER (TSV)')
    parser.add_argument('--gtf',    required=True, help='Fichier GTF GENCODE')
    parser.add_argument('--output', required=True, help='Dossier de sortie')

    parser.add_argument('--mode', choices=['samples', 'all'], default='samples')
    parser.add_argument('--samples', help='Fichier liste de samples')
    parser.add_argument('--partial-match', action='store_true')
    parser.add_argument('--pvalue', type=float)
    parser.add_argument('--gnomad')
    parser.add_argument('--mendeliome')
    parser.add_argument('--no-zip', action='store_true')
    parser.add_argument('--workers', type=int, default=DEFAULT_WORKERS)
    parser.add_argument('--verbose', action='store_true')

    args = parser.parse_args()

    if args.mode == 'samples' and not args.samples:
        parser.error("--samples est requis avec --mode samples")
    if not args.fraser and not args.outrider:
        parser.error("Au moins --fraser ou --outrider est requis")

    if args.verbose:
        logger.setLevel(logging.DEBUG)

    processor = RNASeqProcessorPerSample(
        fraser_file=args.fraser,
        outrider_file=args.outrider,
        samples_file=args.samples,
        gtf_file=args.gtf,
        output_dir=args.output,
        gnomad_file=args.gnomad,
        mendeliome_file=args.mendeliome,
        mode=args.mode,
        pvalue_filter=args.pvalue,
        create_zip=not args.no_zip,
        partial_match=args.partial_match,
        workers=args.workers,
    )

    try:
        processor.run()
        sys.exit(0)
    except Exception as e:
        logger.error(f"Echec : {e}", exc_info=True)
        sys.exit(1)


if __name__ == '__main__':
    main()
